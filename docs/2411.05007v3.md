# SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

**(Paper under double-blind review)**

## Abstract

> Diffusion models can effectively generate high-quality images. However, as they scale, rising memory demands and higher latency pose substantial deployment challenges. In this work, we aim to accelerate diffusion models by quantizing their weights and activations to 4 bits. At such an aggressive level, both weights and activations are highly sensitive, where existing post-training quantization methods like smoothing become insufficient. To overcome this limitation, we propose *SVDQuant*, a new 4-bit quantization paradigm. Different from smoothing, which redistributes outliers between weights and activations, our approach *absorbs* these outliers using a low-rank branch. We first consolidate the outliers by shifting them from activations to weights. Then, we use a high-precision, low-rank branch to take in the weight outliers with Singular Value Decomposition (SVD), while a low-bit quantized branch handles the residuals. This process eases the quantization on both sides. However, naively running the low-rank branch independently incurs significant overhead due to extra data movement of activations, negating the quantization speedup. To address this, we co-design an inference engine *Nunchaku* that fuses the kernels of the low-rank branch into those of the low-bit branch to cut off redundant memory access. It can also seamlessly support off-the-shelf low-rank adapters (LoRAs) without re-quantization. Extensive experiments on SDXL, PixArt-$\Sigma$, and FLUX.1 validate the effectiveness of SVDQuant in preserving image quality. We reduce the memory usage for the 12B FLUX.1 models by 3.5×, achieving 3.0× speedup over the 4-bit weight-only quantization (W4A16) baseline on the 16GB laptop 4090 GPU with INT4 precision. On the latest RTX 5090 desktop with Blackwell architecture, we achieve a 3.1× speedup compared to the W4A16 model using NVFP4 precision. Our [quantization library](https://github.com/mit-han-lab/deepcompressor)[^1] and [inference engine](https://github.com/mit-han-lab/nunchaku)[^2] are open-sourced.
>
> [^1]: Quantization library: [github.com/mit-han-lab/deepcompressor](https://github.com/mit-han-lab/deepcompressor)
> [^2]: Inference Engine: [github.com/mit-han-lab/nunchaku](https://github.com/mit-han-lab/nunchaku)

## 1. Introduction

Diffusion models have shown remarkable capabilities in generating high-quality images (Ho et al., 2020), with recent advances further enhancing user control over the generation process. Trained on vast data, these models can create stunning images from simple text prompts, unlocking diverse image editing and synthesis applications (Meng et al., 2022; Ruiz et al., 2023; Zhang et al., 2023).

[Content of figure_texts/trend.tex]

To pursue higher image quality and more precise text-to-image alignment, researchers are scaling up diffusion models. As shown in [Reference to Figure trend], Stable Diffusion (SD) (Rombach et al., 2022) 1.4 only has 800M parameters, while SDXL (Podell et al., 2023) scales this up to 2.6B parameters. AuraFlow v0.1 (AuraFlow0.1) extends this further to 6B parameters, with the latest model, FLUX.1 (Flux1), pushing the boundary to 12B parameters. Compared to large language models (LLMs), diffusion models are significantly more computationally intensive. Their computational costs[^3] increase more rapidly with model size, posing a prohibitive memory and latency barrier for real-world model deployment, particularly for interactive use cases that demand low latency.

[^3]: Measured by the number of Multiply-Accumulate operations (MACs). 1 MAC=2 FLOPs.

As Moore's law slows down, hardware vendors are turning to low-precision inference to sustain performance improvements. For instance, NVIDIA's Blackwell Tensor Cores introduce a new 4-bit floating point (FP4) precision, doubling the performance compared to FP8 (Blackwell). Therefore, using 4-bit inference to accelerate diffusion models is appealing. In the realm of LLMs, researchers have leveraged quantization to compress model sizes and boost inference speed (Dettmers et al., 2022; Xiao et al., 2023). However, unlike LLMs--where latency is primarily constrained by loading model weights on modern GPUs, especially with small batch sizes--diffusion models are heavily computationally bounded, even with a single batch. As a result, weight-only quantization cannot accelerate diffusion models. To achieve speedup on these devices, both weights and activations must be quantized to the same bit width; otherwise, the lower-precision weight will be upcast during computation, negating potential performance enhancements.

In this work, we focus on quantizing both the weights and activations of diffusion models to 4 bits. This challenging and aggressive scheme is often prone to severe quality degradation. Existing methods like smoothing (Xiao et al., 2023; Lin et al., 2024awq), which transfer the outliers between the weights and activations, are less effective since both sides are highly vulnerable to outliers. To address this issue, we propose a general-purpose quantization paradigm, *SVDQuant*. Our core idea is to use a low-cost branch to absorb outliers on both sides. To achieve this, as illustrated in [Reference to Figure intuition], we first aggregate the outliers by migrating them from activation $\mX$ to weight $\mW$ via smoothing. Then we apply Singular Value Decomposition (SVD) to the updated weight, $\hat{\mW}$, decomposing it into a low-rank branch $\mL_1 \mL_2$ and a residual $\hat{\mW} - \mL_1 \mL_2$. The low-rank branch operates at 16 bits, allowing us to quantize only the residual to 4 bits, significantly reducing outliers and magnitude. However, naively running the low-rank branch separately incurs substantial memory access overhead, offsetting the speedup of 4-bit inference. To overcome this, we co-design a specialized inference engine *Nunchaku*, which fuses the low-rank branch computation into the 4-bit quantization and computation kernels. This design enables us to achieve measured inference speedup even with additional branches.

SVDQuant can quantize various text-to-image diffusion architectures into 4 bits, including both UNet (Ho et al., 2020; Ronneberger et al., 2015) and DiT (Peebles et al., 2023) backbones, while maintaining visual quality. It supports both INT4 and FP4 data types and integrates seamlessly with pre-trained low-rank adapters (LoRA) (Hsu et al., 2022) without requiring re-quantization. To our knowledge, we are the first to successfully apply 4-bit post-training quantization to both the weights and activations of diffusion models, and achieve measured speedup on NVIDIA GPUs. On the latest 12B FLUX.1, our 4-bit models largely preserve the image quality and reduce the memory footprint of the original BF16 model by 3.5×. Furthermore, our INT4 and FP4 model delivers a 3.0× and 3.1× speedup over the NF4 weight-only quantized baseline on the 16GB laptop-level RTX 4090 and desktop-level RTX 5090 GPU, respectively. See [Reference to Figure teaser] for visual examples.

[Content of figure_texts/intuition.tex]

## 2. Quantization Preliminary

Quantization is an effective approach to accelerate linear layers in networks. Given a tensor $\mX$, the quantization process is defined as:

```latex
\begin{equation} \label{eq:quantization_def}
    \mQ_\mX = \text{round}\left(\frac{\mX}{s_{\mX}}\right), s_{\mX} = \frac{\max (|\mX|)}{q_{\max}}.
\end{equation}
```

Here, $\mQ_\mX$ is the low-bit representation of $\mX$, $s_{\mX}$ is the scaling factor, and $q_{\max}$ is the maximum quantized value. For signed $k$-bit integer quantization, $q_{\max} = 2^{k-1}-1$. For 4-bit floating-point quantization with 1-bit mantissa and 2-bit exponent, $q_{\max} = 6$. Thus, the dequantized tensor can be formulated as $Q(\mX) = s_{\mX} \cdot \mQ_\mX$. For a linear layer with input $\mX$ and weight $\mW$, its computation can be approximated by

```latex
\begin{equation}
    \mX \mW \approx Q(\mX)Q(\mW) = s_{\mX}s_{\mW} \cdot \mQ_\mX \mQ_\mW.
\end{equation}
```

The same approximation applies to convolutional layers. To speed up computation, modern commercial GPUs require both $\mQ_\mX$ and $\mQ_\mW$ using the same bit width. Otherwise, the low-bit weights need to be upcast to match the higher bit width of activations, or vice versa, negating the speed advantage. Following the notation in QServe (Lin et al., 2025qserve), we denote $x$-bit weight, $y$-bit activation as W$x$A$y$. ``INT'' and``FP'' refer to the integer and floating-point data types, respectively.

In this work, we focus on W4A4 quantization for acceleration, where outliers in both weights and activations place substantial obstacles.
Traditional methods to suppress these outliers include quantization-aware training (QAT) (HeEfficientDM) and rotation (Ashkboos et al., 2024; Liu et al., 2024spinquant; Lin et al., 2025qserve). QAT requires massive computing resources, especially for tuning models with more than 10B parameters such as FLUX.1. Rotation is inapplicable due to the usage of adaptive normalization layers (Peebles et al., 2023) in diffusion models. The runtime-generated normalization weights preclude the offline rotation with the weights of projection layers, while online rotation of both activations and weights incurs significant runtime overhead.

## 3. Related Work

**Diffusion models** (Sohl-Dickstein et al., 2015; Ho et al., 2020) have emerged as a powerful class of generative models, known for generating high-quality samples by modeling the data distribution through an iterative denoising process. Recent advancements in text-to-image diffusion models (Balaji et al., 2022; Rombach et al., 2022; Podell et al., 2023) have already revolutionized content generation. Researchers further shifted from convolution-based UNet architectures (Ronneberger et al., 2015; Ho et al., 2020) to transformers (Peebles et al., 2023; Bao et al., 2023) and scaled up the model size (Esser et al., 2024). However, diffusion models suffer from extremely slow inference speed due to their long denoising sequences and intense computation. To address this, various approaches have been proposed, including few-step samplers (Zhang et al., 2022fast; Zhang et al., 2022gddim; Lu et al., 2022) or distilling fewer-step models from pre-trained ones (Salimans et al., 2021; Meng et al., 2022distillation; Song et al., 2023; Luo et al., 2023; Sauer et al., 2023; Yin et al., 2024one; Yin et al., 2024improved; Kang et al., 2024). Another line of works choose to optimize or accelerate computation via efficient architecture design (Li et al., 2023snapfusion; Li et al., 2020gan; Cai et al., 2024; Liu et al., 2024linfusion), quantization (Shang et al., 2023; Li et al., 2023q), sparse inference (Li et al., 2022efficient; Ma et al., 2024deepcache; Ma et al., 2024learning), and distributed inference (Li et al., 2024distrifusion; Wang et al., 2024; Chen et al., 2024asyncdiff). This work focuses on quantizing the diffusion models to 4 bits to reduce the computation complexity. Our method can also be applied to few-step diffusion models to further reduce the latency (see [Reference to Section Results]).

**Quantization** has been recognized as an effective approach for LLMs to reduce the model size and accelerate inference (Dettmers et al., 2022; Frantar et al., gptq; Xiao et al., 2023; Lin et al., 2025qserve; Lin et al., 2024awq; Kim et al., 2024; Zhao et al., 2024atom). For diffusion models, Q-Diffusion (Li et al., 2023q) and PTQ4DM (Shang et al., 2023) first achieved 8-bit quantization. Subsequent works refined these techniques with approaches like sensitivity analysis (Yang et al., 2023) and timestep-aware quantization (He et al., 2023ptqd; Huang et al., 2024; Liu et al., 2024enhanced; Wang et al., 2024towards). Some recent works extended the setting to text-to-image models (Tang et al., 2023; Zhao et al., 2024mixdq), DiT backbones (Wu et al., 2024), quantization-aware training (HeEfficientDM; Zheng et al., 2024; Wang et al., 2024quest; Sui et al., 2024), video generation (Zhao et al., 2024vidit), and different data types (Liu et al., 2024hq). Among these works, only MixDQ (Zhao et al., 2024mixdq) and ViDiT-Q (Zhao et al., 2024vidit) implement low-bit inference engines and report measured 8-bit speedup on GPUs. In this work, we push the boundary further by quantizing diffusion models to 4 bits, supporting both the integer or floating-point data types, compatible with the UNet backbone (Ho et al., 2020) and recent DiT architecture (Peebles et al., 2023). Our co-designed inference engine, Nunchaku, further ensures on-hardware speedup. Additionally, when applying LoRA to the model, existing methods require fusing the LoRA branch to the main branch and re-quantizing the model to avoid tremendous memory-access overhead in the LoRA branch. Nunchaku cuts off this overhead via kernel fusion, allowing the low-rank branch to run efficiently as a separate branch, eliminating the need for re-quantization.

**Low-rank decomposition** has gained significant attention in deep learning for enhancing computational and memory efficiency (LoRA; Zhao et al., 2024galore; Jaiswal et al., 2024). While directly applying this approach to model weights can reduce the compute and memory demands (Hsu et al., 2022; Yuan et al., 2023; Li et al., 2023losparse), it often leads to performance degradation. Instead, (Yao et al., 2023) combined it with quantization for model compression, employing a low-rank branch to compensate for the quantization error. Low-Rank Adaptation (LoRA) (LoRA) introduces another active line of research using low-rank matrices to adjust a subset of pre-trained weights for efficient fine-tuning.
This has sparked numerous advancements (Dettmers et al., 2023qlora; Guo et al., 2024; Li et al., 2024loftq; Xu et al., 2024; Meng et al., 2024), which combines quantized models with low-rank adapters to reduce memory usage during model fine-tuning. However, our work differs in two major aspects. First, our goal is different, as we aim to accelerate model inference through quantization, while previous works focus on model compression or efficient fine-tuning. Thus, they primarily consider weight-only quantization, resulting in no speedup. Second, as shown in our experiments ([Reference to Figure svd_fusion] and ablation study in [Reference to Section Results]), directly applying these methods not only degrades the image quality, but also introduces significant overhead. In contrast, our method yields much better performance due to our joint quantization of weights and activations and overhead reduction of our inference engine Nunchaku.

## 4. Method

[Reference to Section Method]

In this section, we first formulate our problem and discuss where the quantization error comes from.
Next, we present SVDQuant, a new W4A4 quantization paradigm for diffusion models. Our key idea is to introduce an additional low-rank branch that can absorb quantization difficulties in both weights and activations.
Finally, we provide a co-designed inference engine Nunchaku with kernel fusion to minimize the overhead of the low-rank branches in the 4-bit model.

[Content of figure_texts/distribution]

### 4.1 Problem Formulation

Consider a linear layer with input $\mX \in \sR^{b\times m}$ and weight $\mW \in \sR^{m\times n}$, where $b$ represents the batch size, and $m$ and $n$ denote the input and output channels, respectively. The quantization error can be defined as

```latex
\begin{equation} \label{eq:error_def}
    E(\mX, \mW) = \norm{\mX \mW - Q(\mX)Q(\mW)}_F,
\end{equation}
```

where $\|\cdot\|_F$ denotes Frobenius Norm.

**Proposition 4.1 (Error decomposition)** [Reference to Proposition decomp]
The quantization error can be decomposed as follows:

```latex
\begin{align} \label{eq:err_decomp}
    E(\mX, \mW) \le \norm{\mX}_F\norm{\mW - Q(\mW)}_F + \norm{\mX - Q(\mX)}_F(\norm{\mW}_F+\norm{\mW-Q(\mW)}_F).
\end{align}
```

See [Reference to Appendix proof1] for the proof. From the proposition, we can see that the error is bounded by four elements -- the magnitude of the weight and input, $\norm{\mW}_F$ and $\norm{\mX}_F$, and their respective quantization errors, $\norm{\mW-Q(\mW)}_F$ and $\norm{\mX-Q(\mX)}_F$. To minimize the overall quantization error, we aim to optimize these four terms.

### 4.2 SVDQuant: Absorbing Outliers via Low-Rank Branch

[Reference to Section SVDQuant]

**Migrate outliers from activation to weight.**
Smoothing (Xiao et al., 2023; Lin et al., 2024awq) is an effective approach for reducing outliers. We can smooth outliers in activations by scaling down the input $\mX$ and adjusting the weight matrix $\mW$ correspondingly using a per-channel smoothing factor $\vlambda \in \sR^m$. As shown in [Reference to Figure distribution](a)(c), the smoothed input $\hat{\mX} = \mX \cdot \text{diag}(\vlambda)^{-1}$ exhibits reduced magnitude and fewer outliers, resulting in lower input quantization error. However, in [Reference to Figure distribution](b)(d), the transformed weight $\hat{\mW} = \mW \cdot \text{diag}(\vlambda)$ has a significant increase in both magnitude and the presence of outliers, which in turn raises the weight quantization error. Consequently, the overall error reduction is limited.

**Absorb magnified weight outliers with a low-rank branch.** Our core insight is to introduce a 16-bit low-rank branch to further migrate the weight quantization difficulty. Specifically, we decompose the transformed weight as $\hat{\mW} = \mL_1 \mL_2 + \mR$, where $\mL_1 \in \sR^{m \times r}$ and $\mL_2 \in \sR^{r\times n}$ are two low-rank factors of rank $r$, and $\mR$ is the residual. Then $\mX \mW$ can be approximated as

```latex
\begin{equation} \label{eq:low-rank-compute}
    \mX \mW = \hat{\mX} \hat{\mW} = \hat{\mX} \mL_1\mL_2 + \hat{\mX} \mR \approx \underbrace{\hat{\mX} \mL_1\mL_2}_{\text{16-bit low-rank branch}} + \underbrace{Q(\hat{\mX}) Q(\mR)}_{\text{4-bit residual}}.
\end{equation}
```

Compared to direct 4-bit quantization, i.e., $Q(\hat{\mX}) Q(\mW)$, our method first computes the low-rank branch $\hat{\mX} \mL_1 \mL_2$ in 16-bit precision, and then approximates the residual $\hat{\mX} \mR$ with 4-bit quantization. Empirically, $r \ll \min(m, n)$, and is typically set to 16 or 32. As a result, the additional parameters and computation for the low-rank branch are negligible, contributing only $\frac{mr + nr}{mn}$ to the overall costs. However, it still requires careful system design to eliminate redundant memory access, which we will discuss in [Reference to Section engine].

From Equation \ref{eq:low-rank-compute}, the quantization error can be expressed as

```latex
\begin{equation}
    \norm{\hat{\mX} \hat{\mW}-(\hat{\mX}\mL_1\mL_2+Q(\hat{\mX})Q(\mR))}_F = \norm{\hat{\mX} \mR - Q(\hat{\mX})Q(\mR)}_F = E(\hat{\mX}, \mR), 
\end{equation}
```

where $\mR = \hat{\mW}-\mL_1\mL_2$. According to Proposition [Reference to Proposition decomp], since $\hat{\mX}$ is already free from outliers, we only need to focus on optimizing the magnitude of $\mR$, $\norm{\mR}_F$ and its quantization error, $\norm{\mR - Q(\mR)}_F$.

**Proposition 4.2 (Quantization error bound)** [Reference to Proposition quant]
For any tensor $\mR$ and quantization method described in Equation \ref{eq:quantization_def} as $Q(\mR) = s_\mR \cdot \mQ_\mR $. Assuming the elements of $\mR$ follow a distribution that satisfies the following regularity condition: There exists a constant $c$ such that

```latex
\begin{align} \label{eq:regularity}
    \mathbb{E} \sbr{\max(|\mR|)} \le c\cdot\mathbb{E} \sbr{\norm{\mR}_F} 
\end{align}
```

Then, we have

```latex
\begin{align}
    \mathbb{E} \sbr{\norm{\mR - Q(\mR)}_F} \le \frac{c\sqrt{\text{size}(\mR)}}{q_{\max}} \cdot \mathbb{E}\sbr{\norm{\mR}_F}
\end{align}
```

where $\text{size}(\mR)$ denotes the number of elements in $\mR$. Especially if the elements of $\mR$ follow a normal distribution, Equation \ref{eq:regularity} holds for $c = \sqrt{\frac{\log \rbr{\text{size}(\mR)}\pi}{\text{size}(\mR)}}$.

[Content of figure_texts/singular_values.tex]

See [Reference to Appendix proof2] for the proof. From this proposition, we obtain the intuition that the quantization error $\norm{\mR-Q(\mR)}_F$ is bounded by the magnitude of the residual $\norm{\mR}_F$. Thus, our goal is to find the optimal $\mL_1\mL_2$ that minimizes $\norm{\mR}_F = \norm{\hat{\mW} - \mL_1\mL_2}_F$, which can be solved by Singular Value Decomposition (SVD) (Eckart et al., 1936; Mirsky, 1960). Given the SVD of $\hat{\mW}=\mU\mSigma\mV$, the optimal solution is $\mL_1 = \mU\mSigma_{:,:r}$ and $\mL_2 = \mV_{:r,:}$. [Reference to Figure singular_values] illustrates the singular value distribution of the original weight $\mW$, transformed weight $\hat{\mW}$ and residual $\mR$. The singular values of the original weight $\mW$ are highly imbalanced. After smoothing, the singular value distribution of $\hat{\mW}$ becomes even sharper, with only the first several values being significantly larger. By removing these dominant values, the magnitude of the residual $\mR$ is dramatically reduced, as $\norm{\mR}_F = \sqrt{\sum_{i=r+1}^{\min(m,n)} \sigma_i^2}$, compared to the original magnitude $\norm{\hat{\mW}}_F = \sqrt{\sum_{i=1}^{\min(m,n)} \sigma_i^2}$, where $\sigma_i$ is the $i$-th singular value of $\hat{\mW}$. Furthermore, [Reference to Figure distribution](d)(e) show that $\mR$ exhibits fewer outliers with a substantially compressed value range compared to $\hat{\mW}$. In practice, we further reduce quantization errors by iteratively updating the low-rank branch through decomposing $\mW - Q(\mR)$ and adjusting $\mR$ accordingly for several iterations, and then picking the result with the smallest error.

[Content of figure_texts/svd_fusion.tex]

### 4.3 Nunchaku: Fusing Low-Rank and Low-Bit Branch Kernels

[Reference to Section engine]

Although the low-rank branch introduces negligible computation in theory, running it as a separate branch incurs large latency overhead—approximately 50% of the 4-bit branch latency, as shown in [Reference to Figure svd_fusion](a).
This occurs because, for a small rank $r$, even though the computational cost decreases significantly, the input and output activation sizes remain unchanged, shifting the bottleneck from computation to memory access. This issue worsens, especially when the activation cannot fit into the GPU L2 cache. For example, in the diffusion transformer block, the up projection in the low-rank branch for QKV projection is much slower since its output exceeds the available L2 cache, resulting in the extra DRAM load and store operations. Fortunately, the down projection $\mL_1$ in the low-rank branch shares the same input as the quantization kernel in the low-bit branch, while the up projection $\mL_2$ shares the same output as the 4-bit computation kernel, as illustrated in [Reference to Figure svd_fusion](b). By fusing the down projection with the quantization kernel and the up projection with the 4-bit computation kernel, the low-rank branch can share the activations with the low-bit branch, eliminating the extra memory access and halving the number of kernel calls. As a result, our low-rank branch adds only 5~10% latency, making it nearly cost-free.

## 5. Experiments

### 5.1 Setups

**Models.** We benchmark our methods using FLUX.1 (Flux1), PixArt-$\Sigma$ (Chen et al., 2024pixart), SANA (Xie et al., 2024sana), Stable Diffusion XL (SDXL) (Podell et al., 2023) and SDXL-Turbo (Sauer et al., 2023), including both the UNet (Ronneberger et al., 2015; Ho et al., 2020) and DiT (Peebles et al., 2023) backbones. See [Reference to Appendix Benchmark Models] for more details.

**Datasets.** Following previous works (Li et al., 2023q; Zhao et al., 2024mixdq; Zhao et al., 2024vidit), we randomly sample the prompts in COCO Captions 2024 (Chen et al., 2015) for calibration. To evaluate the generalization capability of our method, we sample 5K prompts from the MJHQ-30K (Li et al., 2024playground) and the summarized Densely Captioned Images (sDCI) (Urbanek et al., 2024) for benchmarking. See [Reference to Appendix Benchmark Datasets] for more details.

**Baselines.**
We compare SVDQuant against the following post-training quantization (PTQ) methods:

* 4-bit NormalFloat (NF4) is an information-theoretically optimal 4-bit data type for weight-only quantization (Dettmers et al., 2023qlora), which assumes that weights follow a normal distribution. We use the community-quantized NF4 FLUX.1 models (Lllyasviel) as the baselines.
* ViDiT-Q (Zhao et al., 2024vidit) uses per-token quantization and smoothing (Xiao et al., 2023) to alleviate the outliers across different batches and tokens and achieves lossless 8-bit quantization on PixArt-$\Sigma$.
* MixDQ (Zhao et al., 2024mixdq) identifies the outliers in the begin-of-sentence token of text embedding and protects them with 16-bit pre-computation. This method enables up to W4A8 quantization with negligible performance degradation on SDXL-Turbo.
* [TensorRT](https://developer.nvidia.com/blog/tensorrt-accelerates-stable-diffusion-nearly-2x-faster-with-8-bit-post-training-quantization/) contains an industry-level PTQ toolkit to quantize the diffusion models to 8 bits. It uses smoothing and only calibrates activations over a selected timestep range with a percentile scheme.

**Metrics.** Following previous works (Li et al., 2022efficient; Li et al., 2024distrifusion), we evaluate image quality and image similarity with respect to the 16-bit models' results. For image quality assessment, we use FID (FID, lower is better) to measure the distribution distance between the generated images and the ground-truth images (Heusel et al., 2017; Parmar et al., 2021). Besides, we employ Image Reward (higher is better) to approximate the human rating of the generated images (Xu et al., 2024imagereward). We use LPIPS (lower is better) to measure the perceptual similarity (Zhang et al., 2018) and Peak Signal Noise Ratio (PSNR, higher is better) to measure the numerical similarity of the images from the 16-bit models. Please refer to our [Reference to Appendix Quality Results] for more metrics (CLIP IQA (Wang et al., 2022exploring), CLIP Score (Hessel et al., 2021) and SSIM[^4]).

[^4]: [https://en.wikipedia.org/wiki/Structural_similarity_index_measure](https://en.wikipedia.org/wiki/Structural_similarity_index_measure)

[Content of table_texts/quality.tex]

**Implementation details.** Please refer to [Reference to Appendix Implementation Details] for more details.

### 5.2 Results

[Reference to Section Results]

**Visual quality results.**
We report the quantitative results in [Reference to Table quality] across various models and precision levels, and show some corresponding 4-bit qualitative comparisons in [Reference to Figure visual]. Among all models, our 8-bit results can perfectly mirror the 16-bit results, achieving PSNR higher than 21, beating all other 8-bit baselines. On FLUX.1-dev, our INT8 PSNR even reaches 27 on MJHQ.

For 4-bit quantization, NVFP4 outperforms INT4, thanks to the native hardware support of smaller microscaling group size on Blackwell. On FLUX.1, our SVDQuant consistently surpasses the NF4 W4A16 baseline regarding all metrics. For the dev variant, our method even exceeds the original BF16 model regarding Image Reward, suggesting stronger human preference. On PixArt-$\Sigma$, while our INT4 method shows slight degradation, our NVFP4 model achieves a comparable score to the FP16 model. This is likely due to PixArt-$\Sigma$'s highly compact model size (600M parameters), which benefits from a smaller group size. Remarkably, our INT4 and NVFP4 models significantly outperform ViDiT-Q's W4A8 results by a large margin across all metrics. Note that our FP16 PixArt-$\Sigma$ model differs slightly from ViDiT's, though both offer the same quality. For fair comparisons, ViDiT-Q's similarity results are calculated using their FP16 results.

For UNet-based models, on SDXL-Turbo, our 4-bit models substantially outperform MixDQ W4A8, and our FID scores are on par with the FP16 models, indicating no quality loss. On SDXL, our INT4 and NVFP4 results achieve comparable quality to TensorRT's W8A8 performance, which represents the 8-bit SoTA. As shown in [Reference to Figure sdxl-appendix] in the Appendix, our visual quality only shows minor degradation.

[Content of figure_texts/visual.tex]

[Content of figure_texts/efficiency.tex]
[Content of figure_texts/lora.tex]
[Content of figure_texts/ablation.tex]

**Memory save and speedup.**
In [Reference to Figure efficiency], we report measured model size, memory savings, and speedup for FLUX.1. Our INT4 and NVFP4 quantization reduce the original transformer size from 22.2 GiB to 6.1 GiB, including a 0.3 GiB overhead due to the low-rank branch, resulting in an overall 3.6× reduction. Since both weights and activations are quantized, compared to the NF4 weight-only-quantized variant, our inference engine Nunchaku even saves more memory footprint. It offers a 3.0× speedup on both desktop- and laptop-level NVIDIA RTX 4090 GPUs with INT4 precision and a 3.1× speedup on the RTX 5090 GPU with NVFP4 precision, compared to both NF4 and the original 16-bit models. Notably, while the original BF16 model requires per-layer CPU offloading on the 16GB laptop 4090, our INT4 model fits entirely in GPU memory, resulting in a 10.1× speedup by avoiding offloading.

**Integrate with LoRA.** Previous quantization methods require fusing the LoRA branches and re-quantizing the model when integrating LoRAs. In contrast, our Nunchaku eliminates redundant memory access, allowing adding a separate LoRA branch. In practice, we can fuse the LoRA branch into our low-rank branch by slightly increasing the rank, further enhancing efficiency. In [Reference to Figure lora], we exhibit some visual examples of applying LoRAs of five different styles ([Realism](https://huggingface.co/XLabs-AI/flux-RealismLora), [Ghibsky Illustration](https://huggingface.co/aleksa-codes/flux-ghibsky-illustration), [Anime](https://huggingface.co/alvdansen/sonny-anime-fixed), [Children Sketch](https://huggingface.co/Shakker-Labs/FLUX.1-dev-LoRA-Children-Simple-Sketch), and [Yarn Art](https://huggingface.co/linoyts/yarn_art_Flux_LoRA)) to our INT4 FLUX.1-dev model. Our INT4 model successfully adapts to each style while preserving the image quality of the 16-bit version. For more visual examples, see [Reference to Appendix LoRA Results]. For FLUX.1-schnell, we further support LoRAs from one-step conditional model pix2pix-turbo (Parmar et al., 2024), enabling additional controls like sketch. An interactive demo is available [here](https://svdquant.mit.edu/flux1-schnell-sketch/).

**Ablation study.** [Reference to Section Ablation study.]
In [Reference to Figure ablation], we present several ablation studies of SVDQuant on PixArt-$\Sigma$. First, both SVD-only and naive quantization perform poorly in the 4-bit setting, resulting in severe quality degradation. While applying smoothing to the quantization slightly improves image quality compared to naive quantization, the results remain unsatisfactory. LoRC (Yao et al., 2023) introduces a low-rank branch to compensate for quantization errors, but this approach is suboptimal, as quantization errors exhibit a well-spread distribution of singular values. Consequently, low-rank compensation fails to effectively mitigate these errors, as discussed in [Reference to Section SVDQuant]. In contrast, we first decompose the weights and quantize only the residual. As demonstrated in [Reference to Figure singular_values], the first several singular values are significantly larger than the rest, allowing us to shift them to the low-rank branch, effectively reducing weight magnitude. Finally, smoothing consolidates the outliers, enabling the low-rank branch to absorb outliers from the activations and substantially improving image quality.

**Trade-off of increasing rank.** Please refer to [Reference to Appendix Trade-off of Increasing Rank] for more details.

## 6. Conclusion

In this work, we introduce a novel 4-bit post-training quantization paradigm, SVDQuant, for diffusion models. It adopts a low-rank branch to absorb the outliers in both the weights and activations, easing the process of quantization. Our inference engine Nunchaku further fuses the low-rank and low-bit branch kernels, reducing memory usage and cutting off redundant data movement overhead. Extensive experiments demonstrate that SVDQuant preserves image quality. Nunchaku further achieves a 3.5× reduction in memory usage over the original 16-bit model and 3.0× speedup over the W4A16 on an NVIDIA RTX 4090 and 5090 GPUs. This advancement enables the efficient deployment of large-scale diffusion models on edge devices, unlocking broader potential for interactive AI applications.

## Acknowledgments

We thank MIT-IBM Watson AI Lab, MIT and Amazon Science Hub, MIT AI Hardware Program, National Science Foundation, Packard Foundation, Dell, LG, Hyundai, and Samsung for supporting this research. We thank NVIDIA for donating the DGX server.

### Changelog

**V1** Initial preprint release.

**V2** Fix a typo.

**V3** ICLR 2025 camera-ready version. Upgrade the models by combining SVDQuant and GPTQ. Update NVFP4 and SANA results.

---

## Appendix

## A. Proofs

### A.1 Proof of Proposition [Reference to Proposition decomp]

**Proposition 4.1**
The quantization error $E(\mX, \mW) = \norm{\mX \mW - Q(\mX)Q(\mW)}_F$ in Equation \ref{eq:error_def} can be decomposed as follows:

```latex
\begin{align}
    E(\mX, \mW) \le \norm{\mX}_F\norm{\mW - Q(\mW)}_F + \norm{\mX - Q(\mX)}_F(\norm{\mW}_F+\norm{\mW-Q(\mW)}_F).
\end{align}
```

[Reference to Appendix proof1]
**Proof**

```latex
\begin{align*}
    & \norm{\mX \mW - Q(\mX)Q(\mW)}_F \\
  = & \norm{\mX \mW - \mX Q(\mW) + \mX Q(\mW) - Q(\mX)Q(\mW)}_F \\
\le & \norm{\mX(\mW - Q(\mW))}_F + \norm{(\mX - Q(\mX))Q(\mW)}_F \\
\le & \norm{\mX}_F\norm{\mW - Q(\mW)}_F + \norm{\mX - Q(\mX)}_F\norm{Q(\mW)}_F \\
\le & \norm{\mX}_F\norm{\mW - Q(\mW)}_F + \norm{\mX - Q(\mX)}_F\norm{\mW - (\mW - Q(\mW))}_F \\
\le & \norm{\mX}_F\norm{\mW - Q(\mW)}_F + \norm{\mX - Q(\mX)}_F(\norm{\mW}_F + \norm{\mW - Q(\mW)}_F).
\end{align*}
```

### A.2 Proof of Proposition [Reference to Proposition quant]

**Proposition 4.2**
For any tensor $\mR$ and quantization method described in Equation \ref{eq:quantization_def} as $Q(\mR) = s_\mR \cdot \mQ_\mR $. Assuming the elements of $\mR$ follow a distribution that satisfies the following regularity condition: There exists a constant $c$ such that

```latex
\begin{align}
    \mathbb{E} \sbr{\max(|\mR|)} \le c\cdot\mathbb{E} \sbr{\norm{\mR}_F}. \label{eq:app-regularity}
\end{align}
```

Then, we have

```latex
\begin{align}
    \mathbb{E} \sbr{\norm{\mR - Q(\mR)}_F} \le \frac{c\sqrt{\text{size}(\mR)}}{q_{\max}} \cdot \mathbb{E}\sbr{\norm{\mR}_F}
\end{align}
```

where $\text{size}(\mR)$ denotes the number of elements in $\mR$. Especially if the elements of $\mR$ follow a normal distribution, Equation \ref{eq:app-regularity} holds for $c = \sqrt{\frac{\log \rbr{\text{size}(\mR)}\pi}{\text{size}(\mR)}}$.
[Reference to Appendix proof2]

**Proof**

```latex
\begin{align*}
    & \norm{\mR - Q(\mR) }_F \\
  = & \norm{\mR - s_\mR \cdot \mQ_\mR }_F \\
  = & \norm{s_\mR\cdot \frac{\mR}{s_{\mR}} -s_\mR \cdot \text{round}\left(\frac{\mR}{s_{\mR}}\right)}_F \\
%  = & |s_\mR| \norm{\text{round}\left(\frac{\mR}{s_{\mR}}\right) - \frac{\mR}{s_{\mR}}}_F \\
  = & |s_\mR| \norm{\frac{\mR}{s_{\mR}} - \text{round}\left(\frac{\mR}{s_{\mR}}\right)}_F.
\end{align*}
```

So,

```latex
\begin{align}
    & \mathbb{E} \sbr{\norm{\mR - Q(\mR)}_F} \nonumber \\
\le & \mathbb{E} \sbr{|s_\mR|}\sqrt{\text{size}(\mR)} \nonumber \\
  = & \frac{\sqrt{\text{size}(\mR)}}{q_{\max}} \cdot \mathbb{E}\sbr{\max (|\mR|)} \nonumber \\
\le & \frac{c\sqrt{\text{size}(\mR)}}{q_{\max}} \cdot \mathbb{E}\sbr{\norm{\mR}_F} \nonumber
\end{align}
```

Especially, if the elements of $\mR$ follows a normal distribution, we have

```latex
\begin{align}
 \mathbb{E}\sbr{\max (|\mR|)} \le \sigma \sqrt{2\log \rbr{\text{size}(\mR)}}  \label{eq:max_gaussian}
\end{align}
```

where $\sigma$ is the std deviation of the normal distribution. Equation \ref{eq:max_gaussian} comes from the maximal inequality of Gaussian variables (Lemma 2.3 in (Massart, 2007)).

On the other hand,

```latex
\begin{align}
    & \mathbb{E}\sbr{\norm{\mR}_F} \nonumber\\
  = & \mathbb{E}\sbr{\sqrt{\sum_{x\in\mR}x^2}} \nonumber \\
\ge & \mathbb{E}\sbr{\frac{\sum_{x\in\mR} |x|}{\sqrt{\text{size}(\mR)}}} \label{eq:cauchy} \\
  = & \sigma\sqrt{\frac{2\text{size}(\mR)}{\pi}} \label{eq:half_normal},
\end{align}
```

where Equation \ref{eq:cauchy} comes from Cauchy-Schwartz inequality and Equation \ref{eq:half_normal} comes from the expectation of half-normal distribution.

Together, we have that for a normal distribution,

```latex
\begin{align*}
&\mathbb{E}\sbr{\max (|\mR|)}\\
\le& \sigma \sqrt{2\log \rbr{\text{size}(\mR)}}\\
\le& \sqrt{\frac{\log \rbr{\text{size}(\mR)}\pi}{\text{size}(\mR)}}\mathbb{E}\sbr{\norm{\mR}_F}.
\end{align*}
```

In other words, Equation \ref{eq:app-regularity} holds for $c = \sqrt{\frac{\log \rbr{\text{size}(\mR)}\pi}{\text{size}(\mR)}}$.

## B. Benchmark Models

[Reference to Appendix Benchmark Models]
We benchmark our methods using the following six text-to-image models:

* FLUX.1 (Flux1) is the SoTA open-sourced DiT-based diffusion model. It consists of 19 joint attention blocks (Esser et al., 2024) and 38 parallel attention blocks (Dehghani et al., 2023), totaling 12B parameters. We evaluate both the 50-step guidance-distilled (FLUX.1-dev) and 4-step timestep-distilled (FLUX.1-schnell) variants.
* PixArt-$\Sigma$ (Chen et al., 2024pixart) is another DiT-based model. Instead of using joint attention, it stacks 28 attention blocks composed of self-attention, cross-attention, and feed-forward layers, amounting to 600M parameters. We evaluate it on the default 20-step setting.
* SANA (Xie et al., 2024sana) is a 1.6B DiT model. It utilizes a 32× compression autoencoder (Chen et al., 2024deep) and replaces Softmax attention with linear attention to accelerate image generation.
* Stable Diffusion XL (SDXL) is a widely-used UNet-based model with 2.6B parameters (Podell et al., 2023). It predicts noise with three resolution scales. The highest-resolution stage is processed entirely by ResBlocks (He et al., 2016), while the other two stages jointly use ResBlocks and attention layers. Like PixArt-$\Sigma$, SDXL uses cross-attention layers for text conditioning. We evaluate it in the 30-step setting, along with its 4-step distilled variant, SDXL-Turbo (Sauer et al., 2023).

## C. Benchmark Datasets

[Reference to Appendix Benchmark Datasets]
To assess the generalization capability of our method, we adopt two distinct prompt sets with varying styles for benchmarking:

* MJHQ-30K (Li et al., 2024playground) consists of 30K samples from Midjourney with 10 common categories, 3K samples each. We randomly select 5K prompts from this dataset to evaluate model performance on artistic image generation.
* Densely Captioned Images (DCI) (Urbanek et al., 2024) is a dataset containing ~8K images with detailed human-annotated captions, averaging over 1,000 words. For our experiments, we use the summarized version (sDCI), where captions are condensed to 77 tokens using large language models (LLMs) to accommodate diffusion models. Similarly, we randomly sample 5K prompts for realistic image generation.

## D. Implementation Details

[Reference to Appendix Implementation Details]
For the 8-bit setting, we use per-token dynamic activation quantization and per-channel weight quantization with a low-rank branch of rank 16. For the 4-bit setting, we adopt per-group symmetric quantization for both activations and weights, along with a low-rank branch of rank 32. INT4 quantization uses a group size of 64 with 16-bit scales. We use NVFP4 for FP4 quantization, which has native hardware support of group size of 16 with FP8 scales on Blackwell GPUs (Nvidia, 2025blockscaling). We use GPTQ (Frantar et al., gptq) to quantize the residual weights. For FLUX.1 models, the inputs of linear layers in adaptive normalization are kept in 16 bits (i.e., W4A16). For other models, key and value projections in the cross-attention are retained at 16 bits since their latency only covers less than 5% of total runtime.

The smoothing factor $\lambda \in \mathbb R^{m}$ is a per-channel vector whose $i$-th element is computed as $\lambda_i = \max(|\mX_{:,i}|)^\alpha / \max(|\mW_{i,:}|)^{1-\alpha}$ following SmoothQuant (Xiao et al., 2023) Here, $\mX \in \mathbb R^{b\times m}$ and $\mW \in \mathbb R^{m\times n}$. It is decided offline by searching for the best migration strength $\alpha$ for each layer to minimize the layer output mean squared error (MSE) after SVD on the calibration dataset.

## E. Additional Results

[Reference to Appendix Additional Results]

### E.1 Visual Quality Results

[Reference to Appendix Quality Results]

We report extra quantitative quality results with additional metrics in [Reference to Table quality-appendix]. Specifically, CLIP IQA (Wang et al., 2022exploring) and CLIP Score (Hessel et al., 2021) assesses the image quality and text-image alignment with CLIP (Radford et al., 2021), respectively. Structural Similarity Index Measure (SSIM) is used to measure the luminance, contrast, and structure similarity of images produced by our 4-bit model against the original 16-bit model. We also visualize more qualitative comparisons in [Reference to Figure flux-dev-appendix], [Reference to Figure fig:flux-schnell-appendix], [Reference to Figure fig:pixart-sigma-appendix], [Reference to Figure fig:sdxl-appendix] and [Reference to Figure fig:sdxl-turbo-appendix].

[Content of table_texts/appendix/quality]

[Content of figure_texts/appendix/flux-dev]
[Content of figure_texts/appendix/flux-schnell]

[Content of figure_texts/appendix/pixart-sigma]
[Content of figure_texts/appendix/sdxl]
[Content of figure_texts/appendix/sdxl-turbo]

### E.2 LoRA Results

[Reference to Appendix LoRA Results]
In [Reference to Figure lora-appendix], we showcase more visual results of applying the aforementioned five community-contributed LoRAs of different styles ([Realism](https://huggingface.co/XLabs-AI/flux-RealismLora), [Ghibsky Illustration](https://huggingface.co/aleksa-codes/flux-ghibsky-illustration), [Anime](https://huggingface.co/alvdansen/sonny-anime-fixed), [Children Sketch](https://huggingface.co/Shakker-Labs/FLUX.1-dev-LoRA-Children-Simple-Sketch), and [Yarn Art](https://huggingface.co/linoyts/yarn_art_Flux_LoRA)) to our INT4 quantized models.

[Content of figure_texts/appendix/lora.tex]

### E.3 Additional Ablation of SVDQuant

[Content of table_texts/appendix/svdquant-ablation.tex]
In [Reference to Table svdquant-ablation], we provide additional quantitative ablation results of SVDQuant on the MJHQ prompt set (Li et al., 2024playground). Across all models, NVFP4 outperforms INT4 due to its native support for smaller microscaling group sizes on Blackwell. SVDQuant leverages a low-rank branch to absorb outliers, significantly enhancing image quality in all settings. Additionally, it can incorporate GPTQ (Frantar et al., gptq) instead of round-to-nearest for weight quantization, further improving quality in most cases. Notably, combining SVDQuant with NVFP4 precision achieves the best results, reaching a PSNR of 21.5 on FLUX.1-dev, closely matching the image quality of the original 16-bit model. In [Reference to Figure svdquant-ablation], we provide qualitative comparisons across different precision settings.

[Content of figure_texts/appendix/svdquant-ablation.tex]

### E.4 Latency Results

In [Reference to Table latency-appendix], we compare FLUX latency on a laptop-level 4090 GPU across different precisions. Compared to INT8, 4-bit quantization delivers a $1.3\times$ speedup. However, without optimization, SVDQuant incurs an 18% overhead due to the low-rank branch. By eliminating redundant memory access, Nunchaku achieves latency comparable to naive INT4.
[Content of table_texts/appendix/latency.tex]

### E.5 Trade-off of Increasing Rank

[Reference to Appendix Trade-off of Increasing Rank]
[Reference to Figure ablation-rank] presents the results of different rank $r$ in SVDQuant on PixArt-$\Sigma$. Increasing the rank from 16 to 64 significantly enhances image quality but increases parameter and latency overhead. In our experiments, we select a rank of 32, which offers a decent quality with minor overhead.

[Content of figure_texts/appendix/ablation-rank]

### E.6 Trade-off between Quality and Bitwidth

We evaluate LPIPS across different bitwidths for various quantization methods on PixArt-$\Sigma$ and FLUX.1-schnell using the MJHQ dataset in [Reference to Figure tradeoff-appendix], with weights and activations sharing the same bitwidth. Following the convention (Xiao et al., 2023; Lin et al., 2024awq; Lin et al., 2025qserve; Li et al., 2023q; Zhao et al., 2024atom; Dettmers et al., 2022), for bitwidths above 4, we apply per-channel quantization; for 4 or below, we use per-group quantization (group size 64). SVDQuant consistently outperforms naive quantization and SmoothQuant. Notably, on PixArt-$\Sigma$ and FLUX.1-schnell, our 4-bit results match 7-bit and 6-bit naive quantization, respectively.

Our SVDQuant can still generate images in the 3-bit settings on both PixArt-$\Sigma$ and FLUX.1-schnell, performing much better than SmoothQuant. Below this precision (e.g., W2A4 or W4A2), SVDQuant cannot produce images either since 2-bit symmetric quantization is essentially a ternary quantization. Prior work (Ma et al., 2024era; Wang et al., 2023bitnet) has shown that ternary neural networks require quantization-aware training even for weight-only quantization to adapt the weights and activations to the low-bit distribution.
[Content of figure_texts/appendix/tradeoff.tex]

## F. Text Prompts

[Reference to Appendix Text Prompts]
Below we provide the text prompts we use in [Reference to Figure lora] (from left to right).
